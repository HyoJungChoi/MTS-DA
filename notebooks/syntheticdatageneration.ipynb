{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that to run this notebook you should first download the multivariate datasets from the UEA repository and unzip them into the ../data directory: http://www.timeseriesclassification.com/dataset.php"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-05T16:39:23.302194Z",
     "start_time": "2019-09-05T16:39:22.675162Z"
    }
   },
   "outputs": [],
   "source": [
    "from sktime.utils.load_data import load_from_tsfile_to_dataframe\n",
    "from sklearn import preprocessing\n",
    "import time\n",
    "import pickle\n",
    "import os\n",
    "import numpy as np\n",
    "from tslearn.metrics import dtw\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import plotly.express as px"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-05T16:39:32.375180Z",
     "start_time": "2019-09-05T16:39:32.371181Z"
    }
   },
   "outputs": [],
   "source": [
    "datasets = ['ArticularyWordRecognition',\n",
    "'AtrialFibrillation',\n",
    "'BasicMotions',\n",
    "'CharacterTrajectories',\n",
    "'Cricket',\n",
    "'DuckDuckGeese',\n",
    "'EigenWorms',\n",
    "'Epilepsy',\n",
    "'ERing',\n",
    "'EthanolConcentration',\n",
    "'FaceDetection',\n",
    "'FingerMovements',\n",
    "'HandMovementDirection',\n",
    "'Handwriting',\n",
    "'Heartbeat',\n",
    "'InsectWingbeat',\n",
    "'JapaneseVowels',\n",
    "'Libras',\n",
    "'LSST',\n",
    "'MotorImagery',\n",
    "'NATOPS',\n",
    "'PEMS-SF',\n",
    "'PenDigits',\n",
    "'PhonemeSpectra',\n",
    "'RacketSports',\n",
    "'SelfRegulationSCP1',\n",
    "'SelfRegulationSCP2',\n",
    "'SpokenArabicDigits',\n",
    "'StandWalkJump',\n",
    "'UWaveGestureLibrary']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-05T16:39:32.582181Z",
     "start_time": "2019-09-05T16:39:32.579182Z"
    }
   },
   "outputs": [],
   "source": [
    "irregular_datasets = ['JapaneseVowels',\n",
    "                      'CharacterTrajectories',\n",
    "                      'SpokenArabicDigits'\n",
    "                     ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-05T16:39:32.759181Z",
     "start_time": "2019-09-05T16:39:32.755180Z"
    }
   },
   "outputs": [],
   "source": [
    "large_datasets =  ['DuckDuckGeese',\n",
    "  'InsectWingbeat',\n",
    "  'PEMS-SF', \n",
    "  'FaceDetection',\n",
    "  'MotorImagery',\n",
    "  'Handwriting',\n",
    "  'Heartbeat',\n",
    "  'PenDigits',\n",
    "  'PhonemeSpectra',\n",
    "  'LSST',\n",
    "  'EigenWorms',\n",
    "  'FingerMovements']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-05T16:39:33.959182Z",
     "start_time": "2019-09-05T16:39:33.955181Z"
    }
   },
   "outputs": [],
   "source": [
    "def ts_df_to_array(ts_df, index):\n",
    "    return np.array([ts_df.iloc[index].iloc[i].values for i in range(len(ts_df.iloc[0]))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-05T16:53:33.255358Z",
     "start_time": "2019-09-05T16:53:33.251390Z"
    }
   },
   "outputs": [],
   "source": [
    "def ts_df_to_arrays(ts_df, swapaxes=False):\n",
    "    arrays=[]\n",
    "    for index in range(len(ts_df)):\n",
    "        array = ts_df_to_array(ts_df, index)\n",
    "        if swapaxes:\n",
    "            arrays.append(np.swapaxes(array,0,1))\n",
    "        else:\n",
    "            arrays.append(array)\n",
    "    return np.array(arrays)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-06T14:35:46.382429Z",
     "start_time": "2019-09-06T14:35:46.369413Z"
    }
   },
   "outputs": [],
   "source": [
    "def softdtw_augment_train_set(x_train, y_train, classes, num_synthetic_ts, max_neighbors=5): \n",
    "    from tslearn.neighbors import KNeighborsTimeSeries\n",
    "    from tslearn.barycenters import softdtw_barycenter\n",
    "    from tslearn.metrics import gamma_soft_dtw\n",
    "    \n",
    "    # synthetic train set and labels \n",
    "    synthetic_x_train = []\n",
    "    synthetic_y_train = []\n",
    "    # loop through each class\n",
    "    for c in classes:\n",
    "        # get the MTS for this class \n",
    "        c_x_train = x_train[np.where(y_train==c)]\n",
    "        if len(c_x_train) == 1 :\n",
    "            # skip if there is only one time series per set\n",
    "            continue\n",
    "        # compute appropriate gamma for softdtw for the entire class\n",
    "        class_gamma = gamma_soft_dtw(c_x_train)\n",
    "        # loop through the number of synthtectic examples needed\n",
    "        generated_samples = 0\n",
    "        while generated_samples < num_synthetic_ts:\n",
    "            # Choose a random representative for the class\n",
    "            representative_indices = np.arange(len(c_x_train))\n",
    "            random_representative_index = np.random.choice(representative_indices, size=1, replace=False)\n",
    "            random_representative = c_x_train[random_representative_index]\n",
    "            # Choose a random number of neighbors (between 1 and one minus the total number of class representatives)\n",
    "            random_number_of_neighbors = int(np.random.uniform(1, max_neighbors, size=1))\n",
    "            knn = KNeighborsTimeSeries(n_neighbors=random_number_of_neighbors+1, metric='softdtw', metric_params={'gamma': class_gamma}).fit(c_x_train)\n",
    "            random_neighbor_distances, random_neighbor_indices = knn.kneighbors(X=random_representative, return_distance=True)\n",
    "            random_neighbor_indices = random_neighbor_indices[0]\n",
    "            random_neighbor_distances = random_neighbor_distances[0]\n",
    "            nearest_neighbor_distance = np.sort(random_neighbor_distances)[1]\n",
    "            random_neighbors = np.zeros((random_number_of_neighbors+1, c_x_train.shape[1], c_x_train.shape[2]), dtype=float)\n",
    "            for j, neighbor_index in enumerate(random_neighbor_indices):\n",
    "                random_neighbors[j,:] = c_x_train[neighbor_index]\n",
    "            # Choose a random weight vector (and then normalize it)\n",
    "            weights = np.exp(np.log(0.5)*random_neighbor_distances/nearest_neighbor_distance)\n",
    "            weights /= np.sum(weights)\n",
    "            # Compute tslearn.barycenters.softdtw_barycenter with weights=random weights and gamma value specific to neighbors\n",
    "            random_neighbors_gamma = gamma_soft_dtw(random_neighbors)\n",
    "            generated_sample = softdtw_barycenter(random_neighbors, weights=weights, gamma=random_neighbors_gamma)\n",
    "            synthetic_x_train.append(generated_sample)\n",
    "            synthetic_y_train.append(c)         \n",
    "            # Repeat until you have the desired number of synthetic samples for each class\n",
    "            generated_samples += 1\n",
    "    # return the synthetic set \n",
    "    return np.array(synthetic_x_train), np.array(synthetic_y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-09T00:44:16.802838Z",
     "start_time": "2019-09-06T14:59:16.817607Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------\n",
      "ArticularyWordRecognition\n",
      "# replicates: 275\n",
      "# dimensions: 9\n",
      "length of series: 144\n",
      "# classes: 25\n",
      "total 'size': 356400\n",
      "Time (s): 3485.125000\n",
      "-----------\n",
      "-----------\n",
      "AtrialFibrillation\n",
      "# replicates: 15\n",
      "# dimensions: 2\n",
      "length of series: 640\n",
      "# classes: 3\n",
      "total 'size': 19200\n",
      "Time (s): 6193.250000\n",
      "-----------\n",
      "-----------\n",
      "BasicMotions\n",
      "# replicates: 40\n",
      "# dimensions: 6\n",
      "length of series: 100\n",
      "# classes: 4\n",
      "total 'size': 24000\n",
      "Time (s): 384.640625\n",
      "-----------\n",
      "-----------\n",
      "Cricket\n",
      "# replicates: 108\n",
      "# dimensions: 6\n",
      "length of series: 1197\n",
      "# classes: 12\n",
      "total 'size': 775656\n",
      "Time (s): 143778.359375\n",
      "-----------\n",
      "-----------\n",
      "Epilepsy\n",
      "# replicates: 137\n",
      "# dimensions: 3\n",
      "length of series: 206\n",
      "# classes: 4\n",
      "total 'size': 84666\n",
      "Time (s): 1791.171875\n",
      "-----------\n",
      "-----------\n",
      "ERing\n",
      "# replicates: 30\n",
      "# dimensions: 4\n",
      "length of series: 65\n",
      "# classes: 6\n",
      "total 'size': 7800\n",
      "Time (s): 39.671875\n",
      "-----------\n",
      "-----------\n",
      "EthanolConcentration\n",
      "# replicates: 261\n",
      "# dimensions: 3\n",
      "length of series: 1751\n",
      "# classes: 4\n",
      "total 'size': 1371033\n",
      "Time (s): 355517.218750\n",
      "-----------\n",
      "-----------\n",
      "HandMovementDirection\n",
      "# replicates: 160\n",
      "# dimensions: 10\n",
      "length of series: 400\n",
      "# classes: 4\n",
      "total 'size': 640000\n",
      "Time (s): 15424.046875\n",
      "-----------\n",
      "-----------\n",
      "Libras\n",
      "# replicates: 180\n",
      "# dimensions: 2\n",
      "length of series: 45\n",
      "# classes: 15\n",
      "total 'size': 16200\n",
      "Time (s): 110.140625\n",
      "-----------\n",
      "-----------\n",
      "NATOPS\n",
      "# replicates: 180\n",
      "# dimensions: 24\n",
      "length of series: 51\n",
      "# classes: 6\n",
      "total 'size': 220320\n",
      "Time (s): 436.140625\n",
      "-----------\n",
      "-----------\n",
      "RacketSports\n",
      "# replicates: 151\n",
      "# dimensions: 6\n",
      "length of series: 30\n",
      "# classes: 4\n",
      "total 'size': 27180\n",
      "Time (s): 51.109375\n",
      "-----------\n",
      "-----------\n",
      "SelfRegulationSCP1\n",
      "# replicates: 268\n",
      "# dimensions: 6\n",
      "length of series: 896\n",
      "# classes: 2\n",
      "total 'size': 1440768\n",
      "Time (s): 65296.781250\n",
      "-----------\n",
      "-----------\n",
      "SelfRegulationSCP2\n",
      "# replicates: 200\n",
      "# dimensions: 7\n",
      "length of series: 1152\n",
      "# classes: 2\n",
      "total 'size': 1612800\n",
      "Time (s): 89440.828125\n",
      "-----------\n",
      "-----------\n",
      "StandWalkJump\n",
      "# replicates: 12\n",
      "# dimensions: 4\n",
      "length of series: 2500\n",
      "# classes: 3\n",
      "total 'size': 120000\n",
      "Time (s): 83105.734375\n",
      "-----------\n",
      "-----------\n",
      "UWaveGestureLibrary\n",
      "# replicates: 120\n",
      "# dimensions: 3\n",
      "length of series: 315\n",
      "# classes: 8\n",
      "total 'size': 113400\n",
      "Time (s): 4696.484375\n",
      "-----------\n"
     ]
    }
   ],
   "source": [
    "for dataset_name in datasets:\n",
    "    if dataset_name in large_datasets + irregular_datasets:\n",
    "        continue\n",
    "    num_synthetic_ts = 1000\n",
    "\n",
    "    print(\"-----------\")\n",
    "    start = time.process_time()\n",
    "    print(dataset_name)\n",
    "    train_x, train_y = load_from_tsfile_to_dataframe(\"../data/%s/%s_TRAIN.ts\" % (dataset_name, dataset_name)) \n",
    "    test_x, test_y = load_from_tsfile_to_dataframe(\"../data/%s/%s_TEST.ts\" % (dataset_name, dataset_name))\n",
    "    \n",
    "    train_x = ts_df_to_arrays(train_x, swapaxes=True)       \n",
    "    test_x = ts_df_to_arrays(test_x, swapaxes=True)\n",
    "    \n",
    "    num_replicates = train_x.shape[0]\n",
    "    print(\"# replicates: %d\" % (num_replicates))\n",
    "    num_dimensions = train_x.shape[2]\n",
    "    print(\"# dimensions: %d\" % (num_dimensions))\n",
    "    len_series = train_x.shape[1]\n",
    "    print(\"length of series: %d\" % (len_series))\n",
    "    num_classes = len(np.unique(train_y))\n",
    "    print(\"# classes: %d\" % (num_classes))\n",
    "    total_size = num_replicates*num_dimensions*len_series\n",
    "    print(\"total 'size': %d\" % (total_size))\n",
    "    \n",
    "    classes = np.unique(train_y)    \n",
    "    synthetic_x_train, synthetic_y_train = softdtw_augment_train_set(train_x, \n",
    "                                                             train_y, \n",
    "                                                             classes,\n",
    "                                                             num_synthetic_ts)\n",
    "    \n",
    "    pickle.dump(synthetic_x_train, open(\"../syntheticdata/%s_softdtw_synthetic_x_train_%d_%d_%s.pkl\" % (dataset_name, num_synthetic_ts, dba_iters, str(limit_N)), 'wb'))\n",
    "    pickle.dump(synthetic_y_train, open(\"../syntheticdata/%s_softdtw_synthetic_y_train_%d_%d_%s.pkl\" % (dataset_name, num_synthetic_ts, dba_iters, str(limit_N)), 'wb'))\n",
    "\n",
    "            \n",
    "    print(\"Time (s): %f\" % (time.process_time() - start))\n",
    "    print(\"-----------\")"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "finalproject",
   "language": "python",
   "name": "finalproject"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
